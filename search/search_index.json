{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TOSTADAS \u2192 Toolkit for Open Sequence Triage, Annotation and DAtabase Submission","text":""},{"location":"#pathogen-annotation-and-submission-pipeline","title":"PATHOGEN ANNOTATION AND SUBMISSION PIPELINE","text":"<p>A portable, open-source pipeline designed to streamline submission of pathogen genomic data to public repositories.  Reducing barriers to timely data submission increases the value of public repositories for both public health decision making and scientific research. TOSTADAS facilitates routine sequence submission by standardizing and automating: </p> <ul> <li>Metadata Validation   </li> <li>Genome Annotation    </li> <li>File submission    </li> </ul> <p>TOSTADAS is designed to be flexible, modular, and pathogen agnostic, allowing users to customize their submission of raw read data, assembled genomes, or both. The current release has been tested with sequence data from Poxviruses and select bacteria. Testing for additional pathogen is planned for future releases.</p> <p>The current release is tested with sequence data from Poxviruses and select bacteria but TOSTADAS is designed to be flexible, modular, and pathogen agnostic, allowing users to customize their submission of raw read data, assembled genomes, or both.</p>"},{"location":"#pipeline-summary","title":"Pipeline Summary","text":""},{"location":"#1-metadata-validation","title":"(1) Metadata Validation","text":"<p>Verifies that user-provided metadata conforms to NCBI standards and match the sequence data file(s), all of which are organized in an Excel spreadsheet (example file). By default, TOSTADAS uses a set of metadata fields appropriate for most pathogen genomic data submissions, but can be configured to accommodate custom metadata fields specific to any use case. A full guide to using custom metadata fields can be found here: Custom Metadata Guide</p>"},{"location":"#2-gene-annotation","title":"(2) Gene Annotation","text":"<p>Optional gene calling and feature annotation of assembled genomes (FASTA) using one of the following:</p> <p>(1) RepeatMasker + Liftoff (viral)</p> <ul> <li>Optimized for variola and mpox genomes, this workflow combines RepeatMasker for annotating repeat motifs and Liftoff to annotate functional regions. Execution requires a reference genome (FASTA) and feature list (GFF3) definition. Modifications likely necessary for use with other pathogens.</li> </ul> <p>(2) VADR (viral)</p> <ul> <li>Annotates genomes using a set of homologous reference models. TOSTADAS comes packaged with support for monkeypox virus and a full list of supported pathogens is available from VADR GitHub Repository.</li> </ul> <p>(3) Bakta (bacterial)</p> <ul> <li>Annotates bacterial genomes and plasmids using Bakta. Execution requires a reference database (found here), which can be downloaded at runtime. All annotation options produce a general feature format file (GFF) and NCBI feature table (TBL) compatible with downstream NCBI submission requirements.</li> </ul>"},{"location":"#3-submission","title":"(3) Submission","text":"<p>Prepare necessary submission files for BioSample, SRA, and/or GenBank depending on the provided inputs and perform optional upload to NCBI via ftp. This workflow was adapted from the SeqSender public database submission pipeline.</p>"},{"location":"#quick-links","title":"\ud83d\ude80 Quick Links","text":""},{"location":"#general-usage","title":"\u2699\ufe0f General Usage","text":"\ud83d\udcd6 Overview 1\ufe0f\u20e3 Installation 2\ufe0f\u20e3 General NCBI Guide 3\ufe0f\u20e3 Submission Guide 4\ufe0f\u20e3 Output 5\ufe0f\u20e3 Parameters 6\ufe0f\u20e3 Profiles"},{"location":"#advanced-usage","title":"\ud83e\uddea Advanced Usage","text":"1\ufe0f\u20e3 Custom Metadata 2\ufe0f\u20e3 User Provided Annotation 3\ufe0f\u20e3 VADR Installation 4\ufe0f\u20e3 Wastewater Submission"},{"location":"#cdc-specific-usage","title":"\ud83c\udfe2 CDC-Specific Usage","text":"\ud83d\udccb Guides CDC User Guide"},{"location":"#help-faq","title":"\ud83d\udca1 Help &amp; FAQ","text":"\u2753 Help \ud83e\udde9 Contribute Get in Touch Contributions Troubleshooting"},{"location":"developer_notes/","title":"Notes for Developers (and Confused Users)","text":""},{"location":"developer_notes/#outline","title":"Outline","text":"<p>The workflows are: 1. BIOSAMPLE_AND_SRA: Performs submission to BioSample and SRA repositories, fetches the reports, aggregates them and updates the metadata Excel file with assigned accession IDs. 2. GENBANK: performs annotation (optional, if <code>$params.annotation</code> is true), and then performs submission to GenBank repository.</p> <p>The user options for \"workflow\" are: 1. <code>--workflow biosample_and_sra</code>: Runs BIOSAMPLE_AND_SRA, then runs the AGGREGATE_SUBMISSIONS subworkflow (fetches reports, aggregates them, updates metadata file) 2. <code>--genbank</code>: Runs GENBANK workflow. It expects <code>--updated_meta_path</code> to point to an Excel file that matches the format of a validated metadata file (output of BIOSAMPLE_AND_SRA).                 It automatically looks for this in the output directory in a subdirectory called <code>final_submission_outputs</code> within the metadata-specific subdirectory (<code>$params.outdir/$params.metadata_basename/$final_submission_outdir</code>) 3. <code>--fetch_accessions</code>: Runs AGGREGATE_SUBMISSIONS. It will look in <code>--outdir</code> for the relevant metadata subdirectory (the basename of your metadata file) and then traverse the batch directories under <code>submission_outputs</code>.                          It fetches the report.xml files for biosample and sra submissions for each batch. It needs your NCBI Center credentials from <code>submission_config.yaml</code> 4. <code>--full_submission</code>: Runs BIOSAMPLE_AND_SRA, then waits for awhile, then runs AGGREGATE_SUBMISSIONS, then runs GENBANK.                          It waits for <code>$params.submission_wait_time</code> seconds, and if <code>$params.submission_wait_time</code> is <code>calc</code>, then it waits for 3 minutes * <code>params.batch_size</code>.                          This is based on rudimentary testing that suggests NCBI takes about 3 minutes per submission to issue accession IDs (for multiple submissions). 5. <code>--update_submission</code>: Runs BIOSAMPLE_UPDATE workflow.  It is used to submit updates to biosample accessions.                            It requires an Excel metadata file with biosample_accession, such as the one output by BIOSAMPLE_AND_SRA here: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir</code>.</p>"},{"location":"developer_notes/#workflow-specific-details-and-notes","title":"Workflow-Specific Details and Notes","text":"<p>The master branch conducts sample submissions one at a time (not in batch).  It has a different command (notably, no <code>dry_run</code> flag and no <code>workflow</code> flag because it only run ones workflow). One CDC team is still using the master branch, and was not ready to test and adapt their workflow for the dev branch (i.e., to handle batch sample submissions) as of August 2025.</p> <p>So dev cannot be merged to master.</p>"},{"location":"developer_notes/#submitting-to-biosample-and-sra","title":"Submitting to BioSample and SRA","text":"<p>This workflow is pretty straightforward.  It has a few standalone processes and two subworkflows (SUBMISSION and AGGREGATE_SUBMISSIONS). The user can submit only to biosample by setting <code>$params.sra = false</code> or to both biosample and sra using <code>$params.sra = true</code>.</p> <ol> <li> <p>METADATA_VALIDATION: Process that expects an Excel file (<code>$params.meta_path</code>), performs validation and outputs tsv files (and an error log).                          Each tsv files contains valid metadata for a number of submissions specified by <code>$params.batch_size</code>.                         Outputs are here: <code>$params.outdir/$params.metadata_basename/$params.validation_outdir/batched_tsvs</code>.  By default, it's: $outdir//validation_outputs/batched_tsvs <li> <p>CHECK_VALIDATION_ERRORS: Process that exits the pipeline if at least one ERROR is found in the validation log.  ERRORs will not pass NCBI submission checks.                         Input: the validation log. Outputs: status (\"OK\" or \"ERROR\"), and pipeline exists if status is \"ERROR\".</p> </li> <li> <p>WRITE_VALIDATED_FULL_TSV: Process that collects the batch tsv files in <code>batched_tsvs</code> and concatenates them into one validated tsv file.                         Input: a list of all batched_tsv files. Output: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/validated_metadata_all_samples.tsv</code>.                         This output file will be used in the AGGREGATE_SUBMISSIONS subworkflow.</p> </li> <li> <p>SUBMISSION: Subworkflow that runs two (2) processes.                Input: a Nextflow channel for one batch. Output: the submission folder (as a Nextflow channel) for that batch.</p> <p>PREP_SUBMISSION: Process that prepares all the submissions. This instantiates biosample and sra class instances, creates submission.xml and submit.ready files, and symlinks fastq files for sra submission.                  This is the script that creates the batch_/ folders.                   Most of its helper functions are in submission_helper.py.                  Input: a tuple containing the list of samples and corresponding batch_id, and the submission config file. Output: a tuple containing the batch directory, and a prep_submission log file. <p>SUBMIT_SUBMISSION: Process that actually submits the folders. Run with <code>$params.dry_run</code> to see what it will do (e.g., \"would upload Folder X to Folder Y via ftp\").             Folder names on NCBI ftp site are constructed based on local names and following NCBI's requirement that each folder ONLY contain one XML, one submit.ready, and (if sra) the relevant raw sequence files.             The folder structure will look like this: (local) /submission_outputs/// \u2192 (remote) submit/Test//             NOTE: if you're submitting both Illumina and Nanopore data to SRA, these have to be in different submission.xml files. Therefore, they need to be in different folders, so they go here:                 (local) /submission_outputs/// \u2192 (remote) submit/Test/_/             Input: a tuple containing containing the batch directory, and the submission config file. Output: a tuple containing the batch directory, and a submission log file. <li> <p>AGGREGATE_SUBMISSIONS: Subworkflow that runs three (3) processes:</p> <p>FETCH_REPORTS: Process that traverses the submission directory and look for the reports for each database inside each batch dir. Parses these XMLs into a batch-specific csv report file.                Publishes the results to <code>$params.submission_outdir</code>                Input: Submission batch directory and submission config file. Output: fetch_submission log and batch report csv file.</p> <p>AGGREGATE_REPORTS: Process that collates the individual batch report csvs into one final report.csv                Input: Collected list (actually a Nextflow channel) of all the report csvs. Output: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/submission_report.csv</code></p> <p>JOIN_ACCESSIONS_WITH_METADATA: Updates the initial Excel file with the accession IDs, which is needed for genbank submission.                Input: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/submission_report.csv</code> (AGGREGATE_REPORTS output)                        <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/validated_metadata_all_samples.tsv</code> (WRITE_VALIDATED_FULL_TSV output)                Output: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/&lt;your_metadata_filename&gt;__updated.xlsx</code></p> </li>"},{"location":"developer_notes/#submitting-to-genbank","title":"Submitting to GenBank","text":"<p>This requires <code>--updated_meta_path</code>. It can be specified in <code>nextflow.config</code>. If not specified, it looks for the output of JOIN_ACCESSIONS_WITH_METADATA (<code>$params.outdir/$params.metadata_basename/$final_submission_outdir/&lt;your_metadata_filename&gt;__updated.xlsx</code>)</p> <p>GENBANK workflow doesn't validate metadata. It is assumed the user will run biosample_and_sra first (because GenBank submission requires a BioSample accession ID).  It validates the fasta file. If <code>$params.annotation = true</code>, it performs annotation as follows. And it performs submission.  It does not fetch the accession IDs because at the time of development, many GenBank submissions are not done via ftp.</p> <ol> <li> <p>CREATE_BATCH_TSVS: Process that creates batch tsv files of size <code>$params.batch_size</code> from the updated metadata file.             This process just replicates what metadata validation outputs because the structure is needed downstream and this was the most straightforward way to do that.             Inputs: Path to the updated Excel file and batch size value. Outputs: Path to all the batch tsv files.</p> </li> <li> <p>GENBANK_VALIDATION: Process that validates the fasta file according to NCBI requirements.             Inputs: Tuple including a path to the original fasta. Outputs: Tuple including a path to the validated fasta, which is the original fasta renamed to <code>&lt;original name&gt;_cleaned.fsa</code></p> </li> </ol> <p>if <code>$params.annotation = true</code> and <code>$params.repeatmasker_liftoff = true</code>:     NOTE: This can take the path to any repeats library and ref files by specifying <code>$params.repeat_library</code>, <code>$params.ref_fasta_path</code>, and <code>$params.ref_gff_path</code>.</p> <ol> <li>REPEATMASKER_LIFTOFF: Subworkflow consisting of three (3) processes             Inputs: validated fasta. Outputs: validated fasta and gff.     REPEATMASKER: Process that runs RepeatMasker.             Inputs: fasta and repeats library. Outputs: all the outputs of repeatmasker (.cat, .gff, .tbl, .masked, .out)     LIFTOFF_CLI: Process that runs liftoff.             Inputs: fasta, reference fasta, reference gff. Outputs: all the outputs of liftoff (fasta, .gff, errors log)     CONCAT_GFFS: Process that joins the annotations from RepeatMasker and Liftoff.             Inputs: Tuple containing the fasta, the repeatmasker gff, and the liftoff gff; and a reference gff. Outputs: .gff, .tbl, errors log</li> </ol> <p>if <code>$params.annotation = true</code> and <code>$params.vadr = true</code>:     NOTE: The vadr model library can be specified using <code>$params.vadr_models_dir</code>, BUT it uses <code>$params.virus_subtype</code> as the value for <code>mkey</code>.      Run the vadr container, and run <code>v-annotate.pl -h</code> for more details on what <code>mkey</code> is, and see the VADR_ANNOTATION process (<code>--mkey ${params.virus_subtype}</code>).     To add more libraries will require a bit of development effort but is not difficult. </p> <ol> <li>RUN_VADR: Subworkflow consisting of three (3) processes             Inputs: validated fasta. Outputs: .gff, .tbl, errors log.     VADR_TRIM: Process that runs fasta-trim-terminal-ambigs.pl.             Inputs: fasta. Outputs: trimmed.fasta     VADR_ANNOTATION: Process that annotates the fasta using the model library specified.             Inputs: trimmed.fasta and path to vadr models directory. Outputs: path to vadr output files in a folder that has the format: <code>&lt;sample_id&gt;_&lt;virus_subtype&gt;</code>     VADR_POST_CLEANUP: Process that performs final cleanup of annotations             Inputs: path to vadr outputs from VADR_ANNOTATION. Outputs: .gff, .tbl, errors log.</li> </ol> <p>if <code>$params.annotation = true</code> and <code>$params.bakta = true</code> and <code>$params.organism_type = bacteria</code>: 3. RUN_BAKTA: Subworkflow consisting of two (2) processes.  These are the only two nf-core modules in this pipeline.             Inputs: fasta. Outputs: gff and fasta.     BAKTA_BAKTADBDOWNLOAD: Process that downloads the bakta db specified.     BAKTA_BAKTA: Process that annotates the fasta.             Inputs: fasta, path to the bakta db, <code>params.bakta_proteins</code> and <code>params.bakta_prodigal_tf</code>. Outputs: all the bakta output files</p> <ol> <li>SUBMISSION: Subworkflow, same as for submitting to biosample and sra but only runs for genbank.</li> </ol>"},{"location":"developer_notes/#updating-a-biosample-submission","title":"Updating a BioSample Submission","text":"<p>This workflow requires that <code>${params.meta_path}</code> point to a metadata file with the updated biosample fields and a <code>biosample_accession</code> column with a valid Accession ID.  The workflow as-is DOES NOT check the validity of the biosample accession because there is no straightforward way to do that.  Please make sure your accession ID is valid and correct.</p> <p>The workflow also requires <code>${params.original_submission_outdir}</code> which should point to your original NCBI submission for these samples. It is expecting that the original submission was made with Tostadas, so it wants a path ending in <code>submission_outputs</code> (<code>${params.submission_outdir}</code>) here. It's going to look through the batch folders for <code>biosample/submission.xml</code> to validate that certain fields are unchanged, as required by NCBI.</p> <p>It also expects to find the batch_summary.json file from the original submission (in validation_outputs/batched_tsvs) and it uses this file to recreate the same batches as in the original submission. It has to do this in order to validate that certain metadata are unchanged from the original submission, and to update the original submission with the PrimaryId.</p> <p>The workflow runs METADATA_VALIDATION, CHECK_VALIDATION_ERRORS, and WRITE_VALIDATED_FULL_TSV as in BIOSAMPLE_AND_SRA workflow.  After that, it diverges as follows:</p> <ol> <li> <p>REBATCH_METADATA: Process that recreates the original batches to match the data as <code>${params.original_submission_outdir}</code>.         Input: <code>$params.outdir/$params.metadata_basename/$final_submission_outdir/validated_metadata_all_samples.tsv</code> (WRITE_VALIDATED_FULL_TSV output),                  <code>${params.original_submission_outdir}/../${params.validation_outdir}/batched_tsvs/batch_summary.json</code>         Output: paths to the rebatched json and tsv files.</p> </li> <li> <p>UPDATE_SUBMISSION: Process that updates the biosample submission.         Input: a tuple containing the list of samples and corresponding batch_id, and the submission config file, the original submission directory, and the submission config file.         Output: a tuple containing the batch directory, and a prep_submission log file.</p> </li> </ol>"},{"location":"developer_notes/#known-issues-and-idiosyncracies","title":"Known issues and idiosyncracies","text":"<ol> <li> <p>The pipeline doesn't fetch Genbank accession IDs, but it could if they are available.  Doing so will require some optional handling for downstream report csv and updated metadata file generation.</p> </li> <li> <p>The update_submissions workflow was added very late and I didn't have time to rigorously test it. More testing should be done, and additional nf-tests created for the additional processes.</p> </li> <li> <p>I believe a specific line needs to be added to the GenBank XML file (where appropriate) indicating NCBI should perform annotations.  So the pipeline may be to be adjusted such that if there is no gff file provided in the channel, this line gets added to the XML file for appropriate GenBank submissions (i.e., only those that are submitted via ftp). Also, GenBank sqn file needs to be rigorously validated (sars and flu haven't been tested at all).</p> </li> <li> <p>Need some robust checking for the vadr_models_dir vs. species (see notes under annotation).</p> </li> <li> <p>For update_submission, the metadata file is not being copied to the workDir, it's being referenced from its own workDir.  This is not ideal Nextflow coding, and should be changed so that it copies the actual file.    It's happening because of the channel construction (which is being made from a json file in REBATCH_METADATA process). I think this can be pretty easily modified to just output the channel.</p> </li> <li> <p>Outstanding to-do notes in <code>submission_helper.py</code>:          Line 963: These are hard-coded but probably need to be controlled during GENBANK_VALIDATION somehow.         Line 982: This is not an issue, it's actually more of a reminder to me that the way Biosample and SRA XML files get made is different from Genbank (they are called differently in submission_prep.py).          Line 1273: We never did figure out if locus tag prefix can be set automatically. I think it has to be assigned before, which means it must be specified in <code>${params.bakta_locus_tag}</code></p> </li> </ol>"},{"location":"user-guide/cdc-user-guide/","title":"CDC User Guide","text":""},{"location":"user-guide/cdc-user-guide/#environment-setup","title":"Environment Setup","text":""},{"location":"user-guide/cdc-user-guide/#1-clone-the-repository-to-your-local-machine","title":"(1) Clone the repository to your local machine:","text":"<p><code>git clone https://github.com/CDCgov/tostadas.git</code> <code>cd tostadas</code></p>"},{"location":"user-guide/cdc-user-guide/#2-load-the-nextflow-module","title":"(2) Load the Nextflow module:","text":"<p>Initialize the nextflow module by running the following command:</p> <p><code>ml nextflow</code></p>"},{"location":"user-guide/cdc-user-guide/#3-ensure-that-nextflow-is-available-by-running-nextflow-v","title":"(3) Ensure that Nextflow is available by running nextflow -v","text":"<p>Expected Output:</p> <p><code>nextflow version &lt;CURRENT VERSION&gt;</code></p>"},{"location":"user-guide/cdc-user-guide/#4-update-the-default-submissions-config-file-with-your-ncbi-username-and-password-and-run-one-of-the-following-nextflow-commands-to-execute-the-scripts-with-default-parameters-and-the-local-run-environment","title":"(4) Update the default submissions config file with your NCBI username and password, and run one of the following nextflow commands to execute the scripts with default parameters and the local run environment:","text":"<p>See Run a test submission.</p>"},{"location":"user-guide/contributions/","title":"Acknowledgements","text":"<p>The following individuals have contributed directly to the development of the TOSTADAS pipeline:</p> <ul> <li>Kyle O'Connell</li> <li>Jessica Rowell</li> <li>Yesh Kulasekarapandian</li> <li>Ankush Gupta</li> <li>Cole Tindall</li> <li>Ramiya Sivakumar</li> <li>Samantha Sevilla</li> </ul> <p>The following individuals have contributed to the overall design, and testing, of the TOSTADAS pipeline:</p> <ul> <li>Swarnali Louha</li> <li>Michael Desch</li> <li>Ethan Hetrick</li> <li>Nick Johnson</li> <li>Kristen Knipe</li> <li>Shatavia Morrison</li> <li>Yuanyuan Wang</li> <li>Michael Weigand</li> <li>Dhwani Batra</li> <li>Jason Caravas</li> <li>Lynsey Kovar</li> <li>Hunter Seabolt</li> <li>Crystal Gigante</li> <li>Christina Hutson</li> <li>Brent Jenkins</li> <li>Yu Li</li> <li>Ana Litvintseva</li> <li>Matt Mauldin</li> <li>Dakota Howard</li> <li>Ben Rambo-Martin</li> <li>James Heuser</li> <li>Justin Lee</li> <li>Mili Sheth</li> </ul>"},{"location":"user-guide/custom_metadata_guide/","title":"Custom Metadata Fields Guide","text":""},{"location":"user-guide/custom_metadata_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction<ul> <li>Summary</li> <li>Input File (General)</li> <li>Input File (Specifics)</li> </ul> </li> <li>How To Run</li> <li>Outputs</li> <li>See Capabilities / Limitations</li> <li>Data Type Casting Assumptions</li> <li>When Do Checks/Changes Not Proceed</li> </ul>"},{"location":"user-guide/custom_metadata_guide/#introduction","title":"Introduction:","text":""},{"location":"user-guide/custom_metadata_guide/#summary","title":"Summary:","text":"<p>TOSTADAS consists of a validation portion of the pipeline (\u2153 major segments) to ensure that metadata is aligned with sample submission constraints for NCBI databases. By default, the pipeline performs general checks and makes appropriate corrections to metadata, but the option exists to extend this core-functionality further for the user. </p>"},{"location":"user-guide/custom_metadata_guide/#input-file-general","title":"Input File (General):","text":"<p>The pipeline will accept a .JSON file with the following structure: * Key = Custom field name * Value = Array consisting of sub-keys/values </p> <p>Each array contains multiple different key/value pairs, where the user can specify different checks and changes to take place for each custom field. </p> <p>Here is an example of the structure: <pre><code>{\n    \"Name of Custom Field 1\": {\n        \"type\": \"\",\n        \"samples\": [],\n        \"replace_empty_with\": \"\",\n        \"new_field_name\": \"\"\n    },\n\n    \"Name of Custom Field 2\": {\n        \"type\": \"\",\n        \"samples\": [],\n        \"replace_empty_with\": \"\",\n        \"new_field_name\": \"\"\n    }\n}\n</code></pre></p>"},{"location":"user-guide/custom_metadata_guide/#input-file-specifics","title":"Input File (Specifics):","text":"<p>Each key/value within a custom metadata field array will correspond to the different ways the user can perform checks and make changes for each custom metadata field. </p> <p>There are currently four properties: * Data Type (\"type\"):     * Specifies the correct data type for the field     * Must be one of the following:          * Integer         * String         * Boolean         * Float     * The pipeline will check the existing data type, and if it does not match the one specified in the JSON file, then it will attempt to cast it over</p> <ul> <li> <p>Samples (\"samples\"):</p> <ul> <li>Specifies the list of samples the user wants to apply these checks/transformations to</li> <li>Must be one of the following: <ul> <li>\"All\" (it will run these checks/transformations for all samples within the batch)</li> <li>Specific names of samples for application </li> </ul> </li> <li> <p>Will accept a single string or a list of strings for either option. Here are few acceptable variations:</p> <pre><code>(1) \"samples\": \"All\"\n(2) \"samples\": [\"All\"]\n(3) \"samples\": [\"FL0000\", \"FL0001\", \"FL0002\"]\n(4) \"samples\": \"FL0000\"\n</code></pre> </li> </ul> </li> <li> <p>Replace Empty Values (\"replace_empty_with\"):</p> <ul> <li>Specifies the desired value the user would like to replace an empty value with</li> <li>The actual value can be any of the following data types: string, number, float, boolean, or empty </li> </ul> </li> <li> <p>New Field Name (\"new_field_name\"):</p> <ul> <li>Specifies the string to replace the existing field name with</li> <li>Please note that the old field name will no longer exist in the final output </li> </ul> </li> </ul> <p>A completed example of a JSON file can be found here: JSON Example. This is the same JSON used for a test profile run.</p>"},{"location":"user-guide/custom_metadata_guide/#how-to-run","title":"How To Run:","text":"<p>There are two Nextflow parameters used: * validate_custom_fields = Toggles custom metadata field checks on/off. Must be set to True if custom field checks are wanted. * custom_fields_file = Path to your JSON file containing custom field names and check/transformation properties for each.</p> <p>** NOTE: The default value for validate_custom_fields is False in the test profile, therefore this must be changed to True if doing a test run. </p> <p>Once the JSON file for custom fields is set up, and the parameters above have been properly populated, the next step is to initiate the typical nextflow run for the pipeline (information can be found in the README.md here: Quick Start) </p>"},{"location":"user-guide/custom_metadata_guide/#outputs","title":"Outputs:","text":"<p>After running metadata validation, with the appropriate Nextflow parameters and your JSON file, there is a .txt log file that is generated as an output named custom_fields_error.txt.</p> <p>This .txt log file contains information about two aspects generally:  * (1) The actual contents within the provided JSON file:     * It will provide information for each custom metadata field in the JSON     * The following is an example log output for this:      <pre><code>test_field_1:\n    Found value(s) in subfield samples for the custom field named test_field_1 that are not all strings... will remove these\n    You specified some sample names that are not present within metadata file: ['FL00234']. Processed all others.\n\ntest_field_2:\n    Found 'all' specified within samples list, AND other values as well. Proceeded with checking all samples in this case.\n\nAfter preliminary checks, valid information for custom field names have been passed in. Will now check these accordingly\n</code></pre></p> <ul> <li>(2) For each sample, the outcome of performing a custom field check on it (if mentioned in any under \"samples\"):<ul> <li>The following is an example of how this information appears and its content: <pre><code>FL0004:\n    test_field_2 not populated. \n    Replaced field name (test_field_2) with new_field_name2\n\nIL0005:\n    test_field_1 value was not string. Converting to string\n    Successfully converted test_field_1 to a string\n    Replaced field name (test_field_1) with new_field_name\n\nNY0006:\n    All custom field checks passed\n</code></pre></li> </ul> </li> </ul> <p>The custom_fields_error.txt file will be outputted under the errors directory, which is nested within the validation outputs directory.</p>"},{"location":"user-guide/custom_metadata_guide/#capabilities-limitations","title":"Capabilities / Limitations:","text":"<ul> <li> <p>The custom field name must be populated, it cannot be an empty string within the JSON file. If it is empty, then it will be skipped.</p> </li> <li> <p>Different spelling/shortening/formatting/spacing variations for data types can be captured (i.e. boolean = Boolean = bool = bOoL). It will not capture spaces (i.e. b ool) or words that deviate too far (i.e. trueorfalsething != bool).</p> </li> <li> <p>If the string \"All\" is detected within a list, then all samples will be checked for that custom metadata field no matter what (i.e. \"samples\": ['all', , etc.]). <li> <p>If a sample name specified is not within the metadata sheet, then it will be skipped and captured within the log file.</p> </li> <li> <p>If the \"samples\" key is empty for a custom metadata field, or none of the provided values are strings (if some strings are present, then it will proceed with only those), then it will check ALL samples within the batch by default.</p> </li> <li> <p>If the \"type\" key is empty for a custom metadata field, then it will only check if the field is empty or not.</p> </li>"},{"location":"user-guide/custom_metadata_guide/#data-type-casting-assumptions","title":"Data Type Casting Assumptions","text":"<p>When handling the casting between data types (float, string, integer, and boolean), all assumptions inherent to Python are being utilized (there are no custom deviations from this).</p> <p>Here is a quick overview of how Python will handle the casting between certain data types and corresponding values:</p> <ul> <li> <p>Integer \u2192 Boolean:</p> <ul> <li>Non-zero integer = True </li> <li>Integer equal to 0 = False</li> </ul> </li> <li> <p>Float \u2192 Boolean</p> <ul> <li>Non-zero float = True </li> <li>Float equal to 0.0 = False </li> </ul> </li> <li> <p>Boolean \u2192 Integer:</p> <ul> <li>Boolean equal to True = 1</li> <li>Boolean equal to False = 0</li> </ul> </li> <li> <p>Boolean \u2192 Float:</p> <ul> <li>Boolean equal to True = 1.0</li> <li>Boolean equal to False = 0.0</li> </ul> </li> <li> <p>Float \u2192 Integer:</p> <ul> <li>Removes the decimal portion of the float (i.e. 4.2 to 4)</li> <li>The number is not rounded, therefore even 4.8 will be converted to 4 (same as 4.2 above)</li> </ul> </li> <li> <p>Boolean / Integer / Float \u2192 String:</p> <ul> <li>For all casts to string, the literal value will be used as the final string. Here are a few examples:<ul> <li>True to \"True\"</li> <li>0 to \"0\"</li> <li>0.0 to \"0.0\"</li> </ul> </li> </ul> </li> <li> <p>String \u2192 Float / Integer:</p> <ul> <li>Casts the literal representation of the value to a float number </li> <li>If the string is a whole number, but converting it to a float (string \u2192 float), then it will automatically append the value with \".0\" (i.e. \"8\" to 8.0)</li> </ul> <p>** NOTE: if you are doing string \u2192 integer and provide a float literal (\"8.0\"), then Python will NOT be able to convert it to 8</p> </li> </ul>"},{"location":"user-guide/custom_metadata_guide/#when-do-checkschanges-not-proceed","title":"When Do Checks/Changes Not Proceed","text":"<p>The only time custom checks do not proceed is when any one of the following statements are true: * (1) Custom field name is empty</p> <ul> <li> <p>(2) There is at least one valid sample name provided (non-empty string) AND none of the sample(s) listed are in the metadata sheet</p> <p>** NOTE: It is assumed that the sample(s) provided were intentional and not a consistent data type error or else, therefore for the latter case, all samples will be checked as a fail safe</p> </li> </ul>"},{"location":"user-guide/general_NCBI_submission_guide/","title":"NCBI Databases Overview","text":""},{"location":"user-guide/general_NCBI_submission_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>General<ul> <li>What is NCBI</li> <li>NCBI Center Account</li> <li>Key NCBI Repositories TOSTADAS Supports</li> </ul> </li> <li>More Information For Each Database</li> </ul>"},{"location":"user-guide/general_NCBI_submission_guide/#general","title":"General","text":"<p>TOSTADAS lets users submit samples to various NCBI databases with ease. For many of the databases, the pipeline leverages FTP communication to submit samples in an automated manner. TOSTADAS creates many custom log files locally for the submission process and returns valuable information / documents created at the NCBI endpoint as well. Through frequent conversations with personnel from NCBI, TOSTADAS will be continuously updated with any improvments to existing submission mechanisms and/or the implementation of completely new ones from NCBI, in order to provide the best experience for our users.</p>"},{"location":"user-guide/general_NCBI_submission_guide/#what-is-ncbi","title":"What is NCBI?","text":"<p>The National Center for Biotechnology Information (NCBI) is a division of the National Library of Medicine (NLM) at the National Institutes of Health (NIH). NCBI plays a crucial role in advancing bioinformatics, genomics, and computational biology. Its primary mission is to provide access to and facilitate the use of a vast array of biomedical and genomic information.</p> <p>NCBI continues to evolve, offering a wide range of tools and resources to support researchers, healthcare professionals, and the broader scientific community in accessing and utilizing biological information.</p>"},{"location":"user-guide/general_NCBI_submission_guide/#ncbi-center-account","title":"NCBI Center Account","text":"<p>To submit to NCBI using TOSTADAS, you first need to establish an account with NCBI. If you're submitting on behalf of a group (e.g., a CDC branch, or a state Public Health Lab), you will want to create one account for your center to use. NCBI has information on how to create an account here.  You may already have a personal NCBI account, but you should create a Center-level account.  You will need to configure the TOSTADAS submission config file with your NCBI account username and password to facilitate submissions via ftp.</p> <p>TO create a Center Account:</p> <pre><code>*   Contact the following e-mail for account creation: sra@ncbi.nlm.nih.gov and provide the following information:\n    *   Suggested center abbreviation (16 char max)\n    *   Center name (full), center URL &amp; mailing address (including country and postcode)\n    *   Phone number (main phone for center or lab)\n    *   Contact person (someone likely to remain at the location for an extended time)\n    *   Contact email (ideally a service account monitored by several people)\n    *   Whether you intend to submit via FTP or command line Aspera (ascp)\n*   Gain access to an upload directory: Following center account creation, a test area and a production area will be created. Deposit the XML file and related data files into a directory and follow the instructions SRA provides via email to indicate when files are ready to trigger the pipeline.\n*   GISAID: GISAID support is not yet implemented but it may be added in the future.\n</code></pre>"},{"location":"user-guide/general_NCBI_submission_guide/#key-ncbi-repositories-tostadas-supports","title":"Key NCBI Repositories TOSTADAS Supports:","text":""},{"location":"user-guide/general_NCBI_submission_guide/#1-bioproject-biosample","title":"1. BioProject / BioSample:","text":"<ul> <li>Description: BioProject and BioSample are databases that organize and store information about biological projects and samples, respectively, providing context for genomic data submissions.</li> <li>URL: BioProject / BioSample</li> </ul>"},{"location":"user-guide/general_NCBI_submission_guide/#2-sra","title":"2. SRA:","text":"<ul> <li>Description: SRA is a repository that archives and provides access to raw sequence data, including next-generation sequencing data, facilitating the exploration of genomic datasets.</li> <li>URL: SRA</li> </ul>"},{"location":"user-guide/general_NCBI_submission_guide/#3-genbank","title":"3. GenBank:","text":"<ul> <li>Description: GenBank is a DNA sequence database that collects and archives genomic data from researchers worldwide. It plays a pivotal role in the sharing and dissemination of genetic information.</li> <li>URL: General GenBank Docs</li> <li>URL2: Formatting for GenBank</li> </ul>"},{"location":"user-guide/general_NCBI_submission_guide/#more-information-for-each-database","title":"More Information For Each Database","text":"<p>Each database under NCBI has different functions/use-cases, and therefore each requires a unique set of files, as well as formatting/content properties for each. </p> <p>It's important to note that the specific requirements for data submission to these databases can evolve, and it's recommended to refer to the latest guidelines provided by the National Center for Biotechnology Information (NCBI) or the respective databases for the most up-to-date information.</p> Database Minimum Required Files Optional Files Required Metadata Fields Optional Metadata Fields Current Submission Mechanisms SRA (Sequence Read Archive) Raw sequence data files (e.g., FASTQ, BAM), XML metadata file Quality control reports, Experimental design details Sample name, Organism Yes (Strain, Sex, Developmental Stage, etc.) Web-based submission portal, Command-line tools (e.g., <code>SRA Toolkit</code>), FTP GenBank Nucleotide or protein sequence file (FASTA format), Annotation file (GenBank format as a .tbl or .gff) Sequencing trace files, Supplementary data files Organism, Locus tag Yes (Strain, Taxonomy ID, etc.) BankIt submission tool, Sequin interactive submission tool, table2asn via FTP or email BioSample XML metadata file Additional sample attributes file Sample name, Organism Yes (Strain, Sex, etc.) Web-based submission portal, Submission through BioProject or other NCBI databases Joint BioSample/SRA Raw sequence data files (e.g., FASTQ, BAM), XML metadata file (BioSample and SRA metadata combined) Quality control reports, Experimental design details Sample name, Organism Yes (Strain, Sex, Developmental Stage, etc.) Web-based submission portal, Command-line tools (e.g., <code>SRA Toolkit</code>), FTP"},{"location":"user-guide/get-in-touch/","title":"Get in Touch","text":"<p>If you need to report a bug, suggest new features, or just say \u201cthanks\u201d, open an issue and we\u2019ll try to get back to you as soon as possible!</p>"},{"location":"user-guide/get-in-touch/#steps-to-open-issue-request","title":"Steps to Open Issue Request:","text":""},{"location":"user-guide/get-in-touch/#1-select-appropriate-template","title":"(1) Select appropriate template","text":"<p>Following the link above, there are four options for issue templates and your selection will depend on (1) if you are a user vs maintainer/collaborator and (2) if the request pertains to a bug vs feature enhancement. Please select the template that accurately reflects your situation.</p>"},{"location":"user-guide/get-in-touch/#2-fill-out-necessary-information","title":"(2) Fill out necessary information","text":"<p>Once the appropriate template has been selected, you must fill/answer all fields/questions specified. The information provided will be valuable in getting more information about the issue and any necessary context surrounding it.</p>"},{"location":"user-guide/get-in-touch/#3-submit-the-issue","title":"(3) Submit the Issue","text":"<p>Once all information has been provided, you may now submit it!</p> <p>Please allow for some turnaround time for us to review the issue and potentially start addressing it. If this is an urgent request and you have not heard from us nor see any progress being made after quite some time (longer than a week), feel free to start a discussion (found here: Start New Discussion) mentioning the following:</p> <ul> <li>Issue Number</li> <li>Date Submitted</li> <li>General Background on Bug/Feature</li> <li>Reason for Urgency</li> </ul> <p>And we will get back to you as soon as possible.</p>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Environment Setup</li> <li>Run a test submission</li> <li>Start submitting your own data</li> </ul>"},{"location":"user-guide/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"user-guide/installation/#dependencies","title":"Dependencies:","text":"<ul> <li>Nextflow v. 21.10.3 or newer</li> <li>Compute environment (docker, singularity or conda)</li> </ul> <p>\u2757 Note: If you are a CDC user, please follow the set-up instructions found on this page: CDC User Guide</p>"},{"location":"user-guide/installation/#1-clone-the-repository-to-your-local-machine","title":"(1) Clone the repository to your local machine:","text":"<ul> <li><code>git clone https://github.com/CDCgov/tostadas.git</code></li> </ul> <p>\u2757 Note: If you already have Nextflow installed in your local environment, skip ahead to step 5.</p>"},{"location":"user-guide/installation/#2-install-mamba-and-add-it-to-your-path","title":"(2) Install mamba and add it to your PATH","text":"<p>2a. Install mamba</p> <p>\u2757 Note: If you have mamba installed in your local environment, skip ahead to step 3.</p> <p><code>curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh</code></p> <p><code>bash Mambaforge-$(uname)-$(uname -m).sh -b -p $HOME/mambaforge</code></p> <p>2b. Add mamba to PATH:</p> <p><code>export PATH=\"$HOME/mambaforge/bin:$PATH\"</code></p>"},{"location":"user-guide/installation/#3-install-nextflow-using-mamba-and-the-bioconda-channel","title":"(3) Install Nextflow using mamba and the bioconda Channel","text":"<p><code>mamba install -c bioconda nextflow</code></p>"},{"location":"user-guide/installation/#run-a-test-submission","title":"Run a test submission","text":""},{"location":"user-guide/installation/#1-update-the-default-submissions-config-file-with-your-ncbi-username-and-password","title":"(1) Update the default submissions config file with your NCBI username and password","text":"<p><code># update this config file (you don't have to use vim)</code></p> <p><code>vim conf/submission_config.yaml</code></p>"},{"location":"user-guide/installation/#2-run-the-workflow-with-default-parameters-and-the-local-run-environment","title":"(2) Run the workflow with default parameters and the local run environment","text":"<p><code># test command for virus reads</code></p> <p><code>nextflow run main.nf -profile mpox,test,&lt;singularity|docker|conda&gt; --workflow biosample_and_sra</code></p> <p>The pipeline outputs appear in <code>tostadas/results</code></p>"},{"location":"user-guide/installation/#start-submitting-your-own-data","title":"Start submitting your own data","text":"<p>Create an NCBI Center Account. See NCBI Center Account</p> <p>Choose a workflow and specify your profile or (optionally, for annotation and GenBank submission) an <code>organism_Type</code> and <code>virus_subtype</code>.  See: Putting together the Nextflow command</p> <p>Please read the Submission Guide for important details about parameters you need to specify.  Please especially note the following:  * <code>prod_submission</code>: true/false (for submitting to Test vs. Production server). See Other customizations  * <code>batch_size</code>: for submitting large datasets in chunks. We highly recommend you submit using batches! See Other customizations  * <code>workflow</code>: read how to use TOSTADAS for different types of submissions. See Submitting to Production  * profiles: read about profile shortcuts in Using specific profiles </p>"},{"location":"user-guide/outputs/","title":"Outputs","text":""},{"location":"user-guide/outputs/#pipeline-overview","title":"Pipeline Overview:","text":"<p>The workflow will generate outputs in the following order:</p> <ul> <li>Validation<ul> <li>Responsible for QC of metadata</li> <li>Aligns sample metadata .xlsx to sample .fasta</li> <li>Formats metadata into .tsv format</li> </ul> </li> <li>Annotation<ul> <li>Extracts features from .gff</li> <li>Aligns features</li> <li>Annotates sample genomes outputting .gff</li> </ul> </li> <li>Submission<ul> <li>Formats for database submission</li> <li>This section runs twice, with the second run occurring after a wait time to allow for all samples to be uploaded to NCBI.</li> </ul> </li> </ul>"},{"location":"user-guide/outputs/#output-directory-formatting","title":"Output Directory Formatting:","text":"<p>The outputs are recorded in the directory specified within the nextflow.config file and will contain the following:</p> <ul> <li>validation_outputs (name configurable with <code>validation_outdir</code>)<ul> <li>name of metadata sample file<ul> <li>errors</li> <li>fasta</li> <li>tsv_per_sample</li> </ul> </li> </ul> </li> <li>liftoff_outputs (name configurable with <code>final_liftoff_outdir</code>)<ul> <li>name of metadata sample file<ul> <li>errors</li> <li>fasta</li> <li>liftoff</li> <li>tbl</li> </ul> </li> </ul> </li> <li>vadr_outputs (name configurable with <code>vadr_outdir</code>)<ul> <li>name of metadata sample file</li> <li>errors</li> <li>fasta</li> <li>gffs</li> <li>tbl</li> </ul> </li> <li>bakta_outputs (name configurable with <code>bakta_outdir</code>)<ul> <li>name of metadata sample file</li> <li>fasta</li> <li>gff</li> <li>tbl</li> </ul> </li> <li>submission_outputs (name and path configurable with <code>submission_outdir</code>)<ul> <li>individual_sample_batch_folder<ul> <li>biosample</li> <li>sra</li> <li>genbank</li> <li>log_file</li> </ul> </li> </ul> </li> <li>final_submission_outputs (name and path configurable with <code>final_submission_outdir</code>)<ul> <li>updated_metadata_Excel_file</li> <li>submission_report_file</li> </ul> </li> </ul>"},{"location":"user-guide/outputs/#understanding-pipeline-outputs","title":"Understanding Pipeline Outputs:","text":"<p>The pipeline outputs include:</p> <ul> <li>batch_.tsv files for each sample (one for each sample batch) <li>separate fasta files for each sample</li> <li>separate gff files for each sample</li> <li>separate tbl files containing feature information for each sample</li> <li>submission log files<ul> <li>This output is found in the submission_outputs file in your specified output_directory</li> </ul> </li>"},{"location":"user-guide/parameters/","title":"Parameters","text":"<p>Default parameters are given in the nextflow.config file. This table lists the parameters that can be changed to a value, path or true/false. When changing these parameters pay attention to the required inputs and make sure that paths line-up and values are within range. To change a parameter you may change with a flag after the nextflow command or change them within your nextflow.config file.</p> <ul> <li>Please note the correct formatting and the default calculation of submission_wait_time at the bottom of the params table.</li> </ul>"},{"location":"user-guide/parameters/#input-files","title":"Input Files","text":"Param Description Input Required --ref_fasta_path Reference Sequence file path Yes (path as string) --meta_path Meta-data file path for samples Yes (path as string) --ref_gff_path Reference gff file path for annotation Yes (path as string)"},{"location":"user-guide/parameters/#general-subworkflow","title":"General Subworkflow","text":"Param Description Input Required --submission Toggle for running submission Yes (true/false as bool) --annotation Toggle for running annotation (only runs in genbank workflow) Yes (true/false as bool) --dry_run Simulate submission and print a log. No (true/false) --workflow Specifies the workflow to execute, allowing users to choose the appropriate processing method. Yes (string)"},{"location":"user-guide/parameters/#workflow-options","title":"Workflow Options","text":"<p>The following workflows are available for the <code>--workflow</code> parameter:</p> <ul> <li>biosample_and_sra: Runs a submission to BioSample and SRA.</li> <li>genbank: Runs a GenBank submission.</li> <li>fetch_accessions: Fetches reports and updates the metadata file.</li> <li>full_submission: Executes BioSample and SRA submissions, waits 60 seconds multiplied by <code>params.batch_size</code>, fetches reports, updates the metadata file with accession IDs, and then performs the GenBank submission.</li> <li>update_submission: Executes a BioSample submission using an updated metadata Excel file.</li> </ul> <p>Note: The GenBank submission cannot complete without a BioSample accession ID.</p>"},{"location":"user-guide/parameters/#general-settings","title":"General Settings","text":"Param Description Input Required --date_format_flag Flag to specify the date format. Options: s (default, YYYY-MM), v (verbose, YYYY-MM-DD), o (original, unchanged) Yes (string) --publish_dir_mode Mode for publishing directory, e.g., 'copy' or 'move' Yes (string) --remove_demographic_info Flag to remove demographic info. If true, values in host_sex, host_age, race, ethnicity are set to 'Not Provided' Yes (true/false) --batch_size The number of samples to prepare in one submission file. No (integer) --organism_type Used for annotation and to choose GenBank workflow. Options: bacteria, virus, eukaryote No (integer) --virus_subtype Used for VADR annotation. Options: mpxv, rsv. No (integer)"},{"location":"user-guide/parameters/#general-output","title":"General Output","text":"Param Description Input Required --outdir File path to submit outputs from pipeline Yes (path as string) --overwrite_output Toggle to overwriting output files in directory Yes (true/false as bool) --final_submission_outdir Either name or relative/absolute path for the final outputs from submission report fetching No (string or path)"},{"location":"user-guide/parameters/#validation","title":"Validation","text":"Param Description Input Required --validation_outdir File path for outputs specific to validate sub-workflow Yes (folder name as string) --validate_custom_fields Toggle checks/transformations for custom metadata fields on/off No (true/false as bool) --custom_fields_file Path to the JSON file containing custom metadata fields and their information No (path as string)"},{"location":"user-guide/parameters/#liftoff","title":"Liftoff","text":"Param Description Input Required --final_liftoff_outdir File path to liftoff specific sub-workflow outputs Yes (folder name as string) --lift_print_version_exit Print version and exit the program Yes (true/false) --lift_print_help_exit Print help and exit the program Yes (true/false) --lift_parallel_processes Number of parallel processes to use for liftoff Yes (integer) --lift_child_feature_align_threshold Map only if its child features align with sequence identity greater than this value Yes (float) --lift_unmapped_features_file_name Name of unmapped features file Yes (path as string) --lift_copy_threshold Minimum sequence identity in exons/CDS for which a gene is considered a copy; default is 1.0 Yes (float) --lift_distance_scaling_factor Distance scaling factor; default is 2.0 Yes (float) --lift_flank Amount of flanking sequence to align as a fraction of gene length Yes (float between 0.0 and 1.0) --lift_overlap Maximum fraction of overlap allowed by two features Yes (float between 0.0 and 1.0) --lift_mismatch Mismatch penalty in exons when finding best mapping; default is 2 Yes (integer) --lift_gap_open Gap open penalty in exons when finding best mapping; default is 2 Yes (integer) --lift_gap_extend Gap extend penalty in exons when finding best mapping; default is 1 Yes (integer) --lift_minimap_path Path to minimap if you did not use conda or pip Yes (N/A or path as string) --lift_feature_database_name Name of the feature database, if none, will use ref gff path to construct one Yes (N/A or name as string) --lift_feature_types Path to the file containing feature types Yes (path as string) --lift_coverage_threshold Minimum coverage threshold for feature mapping Yes (float) --repeatmasker_liftoff Flag to enable or disable RepeatMasker and Liftoff steps Yes (true/false)"},{"location":"user-guide/parameters/#vadr","title":"VADR","text":"Param Description Input Required --vadr Toggle for running VADR annotation Yes (true/false as bool) --vadr_outdir File path to vadr specific sub-workflow outputs Yes (folder name as string) --vadr_models_dir File path to models for MPXV used by VADR annotation Yes (folder name as string)"},{"location":"user-guide/parameters/#bakta","title":"BAKTA","text":"<p>Controlling Bakta within TOSTADAS uses parameters of the same name with prefix <code>--bakta_</code>. For more details, visit the Bakta GitHub page.</p> Param Description Input Required --bakta Toggle for running Bakta annotation Yes (true/false as bool) --bakta_db_path Path to Bakta database if user is supplying database No (path to database) --download_bakta_db Option to download Bakta database Yes (true/false) --bakta_db_type Bakta database type (light or full) Yes (string) --bakta_outdir File path to bakta specific sub-workflow outputs Yes (folder name as string) --bakta_min_contig_length Minimum contig size Yes (integer) --bakta_threads Number of threads to use while running annotation Yes (integer) --bakta_genus Organism genus name Yes (N/A or name as string) --bakta_species Organism species name Yes (N/A or name as string) --bakta_strain Organism strain name Yes (N/A or name as string) --bakta_plasmid Name of plasmid Yes (unnamed or name as string) --bakta_locus Locus prefix Yes (contig or name as string) --bakta_locus_tag Locus tag prefix Yes (autogenerated or name as string) --bakta_translation_table Translation table Yes (integer) --bakta_gram Gram type for signal peptide predictions No ('+' '-' '?') --save_reference Option to save the downloaded Bakta database No (true/false)"},{"location":"user-guide/parameters/#submission","title":"Submission","text":"Param Description Input Required --biosample Submit to BioSample Yes (true/false as bool) --sra Submit to SRA Yes (true/false as bool) --submission_outdir Either name or relative/absolute path for the outputs from submission Yes (name or path as string) --final_submission_outdir Either name or relative/absolute path for the final outputs from submission report fetching No (string or path) --prod_submission Whether to submit samples for test or actual production Yes (prod or test as string) --submission_config Configuration file for submission to public repos Yes (path as string) --submission_wait_time Calculated based on sample number (3 * 60 secs * sample_num) integer (seconds) --send_submission_email Toggle email notification on/off Yes (true/false as bool) --submission_mode Mode of submission Yes (string)"},{"location":"user-guide/parameters/#update-submission","title":"Update Submission","text":"<p>| --original_submission_outdir | Either name or relative/absolute path for the outputs from original submission (the one being updated) | Yes (name or path as string) |</p> <p>\u2757 Important note about <code>send_submission_email</code>: An email is only triggered if Genbank is being submitted to AND <code>table2asn</code> is the <code>genbank_submission_type</code>. As for the recipient, this must be specified within your submission config file under 'general' as <code>notif_email_recipient</code>.</p>"},{"location":"user-guide/profile/","title":"More Submission Details","text":""},{"location":"user-guide/profile/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Input Files Required</li> <li>(A) Running Annotation and Submission to GenBank and SRA</li> <li>(B) Running SRA Submission only</li> <li>Understanding Profiles and Environments</li> <li>Perform a Dry Run</li> <li>Submitting to BioSample and/or SRA</li> <li>Submitting to GenBank</li> <li>Fetching NCBI Accession IDs</li> <li>Running Update Submission</li> </ul>"},{"location":"user-guide/profile/#input-files-required","title":"Input Files Required:","text":"<p>\u2757 Note: Currently, the pipeline does not accept input files containing period marks . in their naming convention.</p>"},{"location":"user-guide/profile/#a-running-annotation-and-submission-to-genbank-and-sra","title":"(A) Running Annotation and Submission to GenBank and SRA:","text":"Input files File type Description fasta .fasta Single sample fasta sequence file(s) fastq .fastq Single sample fastq sequence file(s) metadata .xlsx Multi-sample metadata matching metadata spreadsheets provided in input_files ref_fasta .fasta Reference genome to use for the liftoff_submission branch of the pipeline ref_gff .gff Reference GFF3 file to use for the liftoff_submission branch of the pipeline submission_config .yaml configuration file for submitting to NCBI, sample versions can be found in repo <p>All annotation workflows require single sample fasta input files. Input fasta files can contain multiple contigs or chromosomes, but all sequences in the file must come from the same specimen.</p>"},{"location":"user-guide/profile/#b-running-sra-submission-only","title":"(B) Running SRA Submission only:","text":"Input files File type Description fastq .fastq Single sample fastq sequence file(s) metadata .xlsx submission_config .yaml configuration file for submitting to NCBI, sample versions can be found in repo <p>\u2757 This pipeline has been tested with paired-end sequence data.</p> <p>Example metadata Link</p>"},{"location":"user-guide/profile/#understanding-profiles-and-environments","title":"Understanding Profiles and Environments:","text":"<p>Within the nextflow pipeline the <code>-profile</code> parameter is required to specify the computing environment of the run. The options of <code>docker</code>, <code>singularity</code> or <code>conda</code> can passed in. The conda environment is less stable than the docker or singularity. We recommend you choose docker or singularity when running the pipeline.</p> <p>Optionally, the <code>test</code> option can be specified in the <code>-profile</code> parameter. If test is not specified, parameters are read from the nextflow.config file. The test params should remain the same for testing purposes.</p> <p>See more about our custom built-in profiles in Using specific profiles.</p>"},{"location":"user-guide/profile/#perform-a-dry-run","title":"Perform a Dry Run:","text":"<p>For any workflow, you can add <code>--dry_run true</code> to run through all the steps but not actually upload files to NCBI's server.  This option produces a few submission log files you can read to check which folders will be uploaded and where they will be uploaded on the host server.</p>"},{"location":"user-guide/profile/#submitting-to-biosample-andor-sra","title":"Submitting to BioSample and/or SRA:","text":"<p>Use the <code>--workflow biosample_and_sra</code> workflow option to submit to BioSample and SRA.  Turn off SRA submission by specifying <code>--sra false</code>.  Turn off BioSample submission using <code>--biosample false</code></p> <p>Note: The column <code>ncbi-spuid</code> in the metadata template is used as the BioSample SPUID, and the column <code>ncbi-spuid-sra</code> is used as the SRA SPUID.  These two fields need to be unique for each sample. NCBI uses SPUID as temporary linkage IDs to connect a BioSample and corresponding SRA submission.</p> <p>Note: SRA submission supports uploading both Nanopore and Illumina data. These will be processed as separate submission.xml files and separate uploads, as required by NCBI.</p>"},{"location":"user-guide/profile/#submitting-to-genbank","title":"Submitting to GenBank:","text":"<p>Use the <code>--workflow genbank</code> workflow option to submit to GenBank. Please note that a GenBank submission requires a BioSample accession ID assigned by NCBI.  If you successfully ran <code>--workflow biosample_and_sra</code> previously, you can find your updated metadata file in the <code>final_submission_outputs</code> by folder by default.  Check it to make sure your accession IDs were successfully assigned.  Supply this file using <code>--updated_meta_path</code> (NOT <code>--meta_path</code>).  Note: TOSTADAS will automatically search for <code>--updated_meta_path</code> in your <code>--outdir</code> if you don't explicitly provide it.</p> <p>\u2757 Note: you can only submit raw files to SRA, not to Genbank.</p>"},{"location":"user-guide/profile/#fetching-ncbi-accession-ids","title":"Fetching NCBI Accession IDs:","text":"<p>TOSTADAS will go search for and fetch report.xml files, aggregate the results into a csv file, and create an updated metadata Excel file including the validated metadata and accession IDs, if assigned. This report CSV file and updated metadata Excel file are placed in the <code>final_submission_outputs</code> by folder by default.</p> <p>Run this workflow using <code>--workflow fetch_accessions</code>.  Provide the same <code>--outdir</code> and <code>--meta_path</code> you provided for the original submission, as TOSTADAS uses these two parameters to find your submission folder and fetch the corresponding reports. If you change the naming of this folder structure, this workflow will not run.</p>"},{"location":"user-guide/profile/#running-update-submission","title":"Running Update Submission:","text":"<p>NCBI allows UI-less updating of BioSample submissions, and TOSTADAS can do this using the <code>--workflow update_submission</code> workflow option.</p> <p>You need to provide <code>--original_submission_dir</code> (the path to your original submission that you are updating) and <code>--meta_path</code> (the Excel file containing the new data for these same samples). TOSTADAS will validate the metadata, recreate the same batches as in the original submission (using the batch_summary.json created during the first submission), update the original submission file and submit it.</p> <p>It will save the updated submissions as date-stamped batch folders under <code>--outdir</code> and within a subdirectory called the basename of your metadata file.</p> <p>Note: TOSTADAS uses the <code>ncbi-spuid</code> field to match samples in the metadata file and the original submission.xml.  The <code>sample_name</code> field is not preserved in the submission.xml, so it cannot be used as an identifier for this workflow.</p> <p>Note: Please make sure your updated metadata Excel file has a <code>biosample_accession</code> column that contains accurate accession IDs.  TOSTADAS does not check these for accuracy.  Please make sure they are correct.</p>"},{"location":"user-guide/submission_guide/","title":"Submission Guide","text":""},{"location":"user-guide/submission_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Putting together the Nextflow command</li> <li>Choosing a workflow</li> <li>Choosing an organism type and/or virus subtype</li> <li>Using specific profiles</li> <li>Other customizations</li> <li>Submitting to Production</li> <li>Typical example workflow</li> <li>Submission config fields</li> <li>Custom metadata validation and custom BioSample package</li> <li>Built-in BioSample package profiles</li> </ul>"},{"location":"user-guide/submission_guide/#putting-together-the-nextflow-command","title":"Putting together the Nextflow command","text":"<p>Your basic command starts like this: <code>nextflow run main.nf -profile &lt;docker|singularity|conda&gt;</code> but needs to be confiured further. See below.</p>"},{"location":"user-guide/submission_guide/#choosing-a-workflow","title":"Choosing a workflow","text":"<p>Choose how you want to run TOSTADAS using the <code>--workflow</code> parameter:</p> <ul> <li>biosample_and_sra: Runs a submission to BioSample and SRA. Add <code>--biosample false</code> or <code>--sra false</code> to toggle off submission to one or the other.</li> <li>genbank: Runs a GenBank submission. This requires an updated metadata file that includes <code>biosample_accession</code> as required by NCBI.</li> <li>fetch_accessions: Fetches reports and updates the metadata file.</li> <li>full_submission: Executes BioSample and SRA submissions, waits 60 seconds multiplied by <code>--batch_size</code>, fetches reports, updates the metadata file with accession IDs, and then performs the GenBank submission.</li> <li>update_submission: Executes a BioSample submission using an updated metadata Excel file.</li> </ul>"},{"location":"user-guide/submission_guide/#choosing-an-organism-type-andor-virus-subtype","title":"Choosing an organism type and/or virus subtype","text":"<p>If you want to run viral annotation, you need to specify a <code>--virus_subtype &lt;mpxv|rsv&gt;</code>.  This tells TOSTADAS which annotator profile to use if you're running VADR.</p> <p>If you want to run bacterial annotation, you need to specify <code>--organism_type bacteria</code>. This tells TOSTADAS to annotate using bakta.  You can instead use a profile (see Using specific profiles).</p> <p>If you're submitting to GenBank (the only option if you want to run annotation), you need to specify <code>--organism_type &lt;virus|bacteria|eukaryote&gt;</code>.  This tells TOSTADAS which kind of GenBank submission to do. FTP submission to GenBank is only supported for bacteria and eukaryote assemblies.  Virus assemblies must be submitted via email (either using TOSTADAS or manually emailing the files in the results folder).</p>"},{"location":"user-guide/submission_guide/#using-specific-profiles","title":"Using specific profiles","text":"<p>TOSTADAS supports some profiles to make submission easier.  These are specified in the <code>-profile</code> option. See Custom metadata validation and custom BioSample package for more detail.</p> <ul> <li>test: Runs a test submission. It prepares all the files but does not actually submit to the test server. To submit to the test server, add <code>dry_run false</code></li> <li>nwss: Submits to SARS-CoV-2.wwsurv.1.0 BioSample package.</li> <li>pulsenet: Submits to OneHealthEnteric.1.0 BioSample package. </li> <li>virus: Sets defaults for virus submission (to run a test bacteria submission, use <code>profile test,virus,&lt;docker|singularity|conda&gt;</code>)</li> <li>bacteria: Sets defaults for bacteria submission (to run a test bacteria submission, use <code>profile test,bacteria,&lt;docker|singularity|conda&gt;</code>)</li> <li>mpox: Sets defaults for MPOX submission (to run a test MPOX submission, use <code>profile test,mpox,&lt;docker|singularity|conda&gt;</code>)</li> <li>rsv: Sets defaults for RSV submission (to run a test RSV submission, use <code>profile test,rsv,&lt;docker|singularity|conda&gt;</code>)</li> </ul>"},{"location":"user-guide/submission_guide/#other-customizations","title":"Other customizations","text":"<p>All the custom parameters for TOSTADAS are found in nextflow.config and the config files inside <code>conf/</code>.  You can override any of these by specifying the parameter on the command line.</p> <p>For example, the default output directory is <code>results</code>, but you can override that and choose your own output directory using <code>--outdir path/to/my/output</code> in your command.</p> <p>TOSTADAS can chunk large datasets into smaller groups to submit to NCBI's servers using the <code>--batch_size</code> flag.  If you have a metadata Excel file with 200 samples, you can submit them in batches of 50 by adding <code>--batch_size 50</code> to your command. This groups 50 samples at a time into one submission file for each data repository. NCBI much prefers this over submitting samples one-at-a-time.   </p> <p>We highly recommend you submit using batches!!! We suggest 50 as a maximum batch size.   </p> <p>Another example: the <code>--dry_run</code> flag (which prepares files for submission but doesn't upload to the server) defaults to <code>true</code> for the test profile and <code>false</code> otherwise, but you can override it by specifying <code>--dry_run &lt;true|false&gt;</code> on the command line.</p>"},{"location":"user-guide/submission_guide/#submitting-to-production","title":"Submitting to Production","text":"<p>TOSTADAS defaults to submitting to the test server even if not using the test profile, to avoid accidentally pushing data to NCBI's Production server.   </p> <p>When you've completed testing and are ready to submit for production, add <code>--prod_submission</code> to your command line (or change <code>prod_submission</code> to <code>true</code> in <code>nextflow.config</code>).    </p>"},{"location":"user-guide/submission_guide/#typical-example-workflow","title":"Typical example workflow","text":"<p>We'll run test submissions to BioSample and SRA using the test MPOX data included in the repository.   </p> <p>Submit to biosample and sra:  <code>nextflow run main.nf -profile test,singularity,mpox --workflow biosample_and_sra --dry_run false --submission_config conf/submission_config.yaml --batch_size 5</code> Remember to add credentials to your submission_config.yaml file.   </p> <p>Fetch the accessions if they weren\u2019t assigned (this workflow creates an updated Metadata Excel file with the validated fields and the accession IDs):  <code>nextflow run main.nf -profile test,singularity,mpox --workflow fetch_accessions --dry_run false --submission_config conf/submission_config.yaml</code> </p> <p>Submit an updated biosample submission (open the updated Excel file from results/mpxv_test_metadata/final_submission_outputs/mpxv_test_metadata_updated.xlsx and add some fake SAMN IDs first):  <code>nextflow run main.nf -profile test,singularity --workflow update_submission --dry_run false --species mpxv --submission_config conf/submission_config.yaml --batch_size 5 --original_submission_outdir results/mpxv_test_metadata/submission_outputs --meta_path results/mpxv_test_metadata/final_submission_outputs/mpxv_test_metadata_updated.xlsx</code> Remember This won\u2019t run without those fake SAMN IDs in the biosample_accession field.   </p> <p>Now we'll run a test GenBank submission using the test bacteria data included in the repository.   </p> <p>Submit to BioSample first (because GenBank requires a BioSample accession):  <code>nextflow run main.nf -profile test,singularity,bacteria --workflow biosample_and_sra --dry_run false --submission_config conf/submission_config.yaml</code> </p> <p>Open the updated Excel file from results/bacteria_test_metadata_1/final_submission_outputs/bacteria_test_metadata_1_updated.xlsx and add some fake SAMN IDs first.  The next command won't run without the fake SAMN IDs in biosample_accession column.  <code>nextflow run main.nf -profile test,singularity,bacteria --workflow genbank --dry_run false --submission_config conf/submission_config.yaml --annotation --download_bakta_db --bakta_db_light</code> </p>"},{"location":"user-guide/submission_guide/#submission-config-fields","title":"Submission config fields","text":"<p>The fields and corresponding example values can be found here: Submission Config.</p> Field Name Description Input Required NCBI / username Your personal username credential for NCBI Yes (string) NCBI / password Your personal password credential for NCBI Yes (string) NCBI_ftp_host The FTP host name for NCBI Yes (string) NCBI_sftp_host The SFTP host name for NCBI Yes (string) NCBI_API_URL URL for the NCBI API Yes (string) table2asn_email Email address for GenBank email submission No (string) BioSample_package Name of BioSample package for submission Yes (string) Role Role of person submitting (should be \"owner\") Yes (string) Type Type of submission (should usually be \"institute\") Yes (string) NCBI_Namespace An SPUID attribute that is unique for each submitter, coordinate this with NCBI Yes (string) Org_ID Organization ID for NCBI Yes (string) Submitting_Org Name of the organization or company you are affiliated with Yes (string) Submitting_Org_Dept Name of the department with organization or company No (string) Street Street address of the organization or company Yes (string) City City of the organization or company Yes (string) State State of the organization or company Yes (string) Postal_Code Zip code of the organization or company Yes (string) Country Country of the organization or company Yes (string) Email Submitter's email address Yes (string) Phone Submitter's phone number No (string) Specified_Release_Date Specify a date to release the samples to the public repository No (string) Submitter Leave blank Yes (blank) '@email' Submitter's email address Yes (string) '@alt_email' An alternate email address to also receive NCBI submission notification emails Yes (string) Name Leave blank Yes (blank) First Submitter's first name Yes (string) Last Submitter's last name Yes (string)"},{"location":"user-guide/submission_guide/#custom-metadata-validation-and-custom-biosample-package","title":"Custom metadata validation and custom BioSample package","text":"<p>TOSTADAS defaults to Pathogen.cl.1.0 (Pathogen: clinical or host-associated; version 1.0) NCBI BioSample package for submissions to the BioSample repository. You can submit using a different BioSample package by doing the following:</p> <ol> <li>Change the package name in the <code>conf/submission_config.yaml</code>. Choose one of the available NCBI BioSample packages.</li> <li>Add the necessary fields for your BioSample package to your input Excel file.</li> <li>Add those same fields as keys to the JSON file (<code>assets/custom_meta_fields/example_custom_fields.json</code>) and provide key info as needed. This lets TOSTADAS know to validate and submit those added fields.</li> <li>Tell TOSTADAS to validate this metadata by adding: <code>--custom_fields_file &lt;path/to/metadata_custom_fields.json&gt; --validate_custom_fields</code> to your command.</li> </ol> <p>replace_empty_with: TOSTADAS will replace any empty cells with this value (Example application: NCBI expects some value for any mandatory field, so if empty you may want to change it to \"Not Provided\".)</p> <p>new_field_name: TOSTADAS will replace the field name in your metadata Excel file with this value. (Example application: you get weekly metadata Excel files and they specify 'animal_environment' but NCBI expects 'animal_env'; you can specify this once in the JSON file and it will be changed on every run.)</p> <p>Note: All fields for the BioSample package Pathogen.cl.1.0. are already in the metadata template.</p>"},{"location":"user-guide/submission_guide/#built-in-biosample-package-profiles","title":"Built-in BioSample package profiles","text":"<p>TOSTADAS has built-in profiles for two BioSample packages to support specific programs.  These profiles automatically import a custom_fields JSON file preconfigured for that package. Here's how to use them:</p> <ul> <li>SARS-CoV-2.wwsurv.1.0 <ol> <li>Change the BioSample_package field in <code>conf/submission_config.yaml</code> to <code>SARS-CoV-2.wwsurv.1.0</code></li> <li>Use <code>assets/sample_metadata/wastewater_biosample_template.xlsx</code> as your metadata template</li> <li>Run as: <code>nextflow run main.nf -profile nwss,&lt;docker|singularity&gt; --meta_path &lt;path/to/metadata_file.xlsx&gt; --submission_config &lt;path/to/submission_config.yaml&gt;</code></li> </ol> </li> <li>OneHealthEnteric.1.0<ol> <li>Change the BioSample_package field in <code>conf/submission_config.yaml</code> to <code>OneHealthEnteric.1.0</code></li> <li>Use <code>assets/sample_metadata/onehealth_biosample_package_template.xlsx</code> as your metadata template</li> <li>Run as: <code>nextflow run main.nf -profile pulsenet,&lt;docker|singularity&gt; --meta_path &lt;path/to/metadata_file.xlsx&gt; --submission_config &lt;path/to/submission_config.yaml&gt;</code></li> </ol> </li> </ul>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>If you encounter issues while using the TOSTADAS pipeline, refer to the following troubleshooting steps to resolve common problems:</p>"},{"location":"user-guide/troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/troubleshooting/#1-errors-with-table2asn-not-on-path-or-a-python-library-missing-when-using-the-singularity-or-docker-profiles","title":"1. Errors with 'table2asn not on PATH' or a Python library missing when using the <code>singularity</code> or <code>docker</code> profiles","text":"<p>Issue: Nextflow is using an outdated cached image.</p> <p>Solution: Locate the image (e.g., <code>$HOME/.singularity/staphb-tostadas-latest.img</code>) and delete it. This will force Nextflow to pull the latest version.</p>"},{"location":"user-guide/troubleshooting/#2-pipeline-hangs-indefinitely-during-the-submission-step-or-you-get-a-duplicate-bioseq-id-error","title":"2. Pipeline hangs indefinitely during the submission step, or you get a \"duplicate BioSeq ID error\"","text":"<p>Issue: This may be caused by duplicate sample IDs in the FASTA file (e.g., a multicontig FASTA). This is only a problem for submissions to Genbank using <code>table2asn</code>.</p> <p>Solution: Review the sequence headers in the sample FASTA files and ensure that each header is unique.</p>"},{"location":"user-guide/troubleshooting/#get-in-touch","title":"Get in Touch","text":"<p>If you need to report a bug, suggest new features, or just say \u201cthanks\u201d, open an issue and we\u2019ll try to get back to you as soon as possible!</p>"},{"location":"user-guide/user_provided_annotation_guide/","title":"User Provided Annotation Guide","text":""},{"location":"user-guide/user_provided_annotation_guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Using Table2asn (GenBank)<ul> <li>Gene Annotation Formatting</li> </ul> </li> </ul>"},{"location":"user-guide/user_provided_annotation_guide/#introduction","title":"Introduction","text":"<p>GenBank is the database primarily associated with genome annotations. The minimum required files for GenBank include a nucleotide or protein sequence file (FASTA format) and an annotation file (GenBank format). The annotation file typically contains information about features such as genes, coding regions, and other elements in the sequence.</p> <p>On the other hand, databases like SRA (Sequence Read Archive), BioSample, and Joint BioSample/SRA primarily deal with raw sequence data, metadata about samples, and experimental details. They do not necessarily require genome annotation files.</p> <p>General information about annotation examples can be found here. This documentation provided by NCBI gives information on relevant features based on your sequence/gene type (mRNA, Prokaryote, Eukaryote, Viral, etc.), genomic elements, and types of database submissions that NCBI expects.</p>"},{"location":"user-guide/user_provided_annotation_guide/#using-table2asn-genbank","title":"Using Table2asn (GenBank)","text":"<p>A popular method for GenBank submission is to use table2asn. </p> <p>Table2asn is a command-line program that creates sequence records for submission to GenBank (.sqn file for every .fsa file). This tool outputs an ASN.1 (Abstract Syntax Notation 1) text file with the same basename and a .sqn suffix as well. </p> <p>Required inputs into table2asn are the following (click the name to view required formatting/general information for each input file):  * Template File (.sbt)     * TOSTADAS handles the creation of this based on your metadata information and contents within your submission config file  * FASTA File (.fasta) * Genome Annotation File (.gff or .tbl)     * Please note that either a .gff formatted file (GenBank prokaryotic or eukaryotic genomes can use GFF3 files in a GenBank-specific format) or a .tbl formatted file can be used for your annotations     * Each of these annotation files need to have the same name prefix as its corresponding .fasta file (i.e. helicase.fsa and helicase.tbl) (i.e. helicase.fsa and helicase.gff)</p>"},{"location":"user-guide/user_provided_annotation_guide/#gene-annotation-formatting","title":"Gene Annotation Formatting","text":"<p>In order to successfully submit samples to GenBank using table2asn, specific requirements for formatting and content must be followed. </p> <p>The requirements for GFF3 annotation files can be found here.</p>"},{"location":"user-guide/vadr_install/","title":"VADR Install Guide for Biolinux","text":""},{"location":"user-guide/vadr_install/#1-clone-the-repo","title":"1. Clone the Repo","text":"<p>First, make sure that you are inside of the root directory of the tostadas repository, and then run the following:</p> <pre><code>git clone https://github.com/ncbi/vadr.git\ncd vadr\nVADRINSTALLDIR=$PWD\n</code></pre>"},{"location":"user-guide/vadr_install/#2-run-the-install-script-if-getting-an-error-with-curl-install-curl-using-mamba","title":"2. Run the Install Script, If Getting an Error with Curl, Install Curl Using Mamba","text":"<p><code>mamba install -c conda-forge curl</code></p> <p>Windows: <pre><code>bash vadr-install.sh linux\n</code></pre> Mac: <pre><code>bash vadr-install.sh macosx\n</code></pre> Troubleshooting:</p> <p>If you receive the following error message: <pre><code>vadr-install.sh: line 174: autoconf: command not found\n</code></pre> Then, install autoconf using either of the following commands:</p> <p><pre><code>sudo apt-get install autoconf\n</code></pre> OR <pre><code>brew install autoconf\n</code></pre></p>"},{"location":"user-guide/vadr_install/#3-set-up-the-mpxv-model-directory","title":"3. Set Up the MPXV Model Directory","text":"<p><pre><code>curl https://ftp.ncbi.nlm.nih.gov/pub/nawrocki/vadr-models/mpxv/1.4.2-1/vadr-models-mpxv-1.4.2-1.tar.gz --output mpxv-models.tar.gz\ntar -xf mpxv-models.tar.gz &amp;&amp; mv vadr-models-mpxv-* mpxv-models\n</code></pre> You also need to copy the modified model file that includes the ITRs from our MPXV repo. <code>cp ../vadr_files/mpxv.rpt.minfo mpxv-models/</code></p>"},{"location":"user-guide/vadr_install/#4-export-paths","title":"4. Export PATHS","text":"<p>Or add to your .bashrc profile</p> <p>Use env_variables.sh file to export path variables: <pre><code>cd .. &amp;&amp; . vadr_files/env_variables.sh\n</code></pre></p>"},{"location":"user-guide/vadr_install/#5-test-and-troubleshoot-the-install","title":"5. Test and Troubleshoot the Install","text":"<p>Test your install by running <code>perl vadr/v-annotate.pl</code>. </p> <p>It will probably fail with either (1) <code>use: command not found</code> or (2) <code>Cant locate XYZ package in @INC</code>. </p> <p>If it works as expected, skip to #5 to export your variables. Else, you can now begin the process of troubleshooting by installing the required PERL libraries.</p> <p>To Install Bio/Easel/MSA.pl <pre><code>perl vadr/Bio-Easel/Makefile.PL\ncd vadr/Bio-Easel\nmake\nmake install\n</code></pre> ** If <code>make install</code> throws a permission error, try running it as admin with <code>sudo make install</code> instead</p> <p>To Install LWP/Simple.pm <pre><code>cpan install LWP\n</code></pre> If error about sqp_opts.pm copy the files in sequip to your Perl path, which is shown as the @INC <code>cp sequip/* &lt;YOUR_PATH&gt;</code></p>"},{"location":"user-guide/vadr_install/#6-run-the-annotation-script","title":"6. Run the annotation script","text":"<p>Ideally you have as many threads as samples, since VADR will give one thread to each sample <pre><code>perl vadr/v-annotate.pl --split --cpu 8 --glsearch --minimap2 -s -r --nomisc \\\n--r_lowsimok --r_lowsimxd 100 --r_lowsimxl 2000 --alt_pass \\\ndiscontn,dupregin --s_overhang 150 -i vadr/mpxv-models/mpxv.rpt.minfo -n \\\nvadr/mpxv-models/mpxv.fa -x $MDIR input_files/trialDatav5.fasta \\\nvadr_testing_outdir -f\n</code></pre></p>"},{"location":"user-guide/wastewater_guide/","title":"NWSS Sequence Submission User Guide","text":""},{"location":"user-guide/wastewater_guide/#overview","title":"Overview","text":"<p>This workflow uses Nextflow to automate submission of FASTQ read files to NCBI's SRA database. It includes three steps.  + Metadata validation: Check that your Excel data conforms to NCBI expectations + Biosample submission: Submit each sample to Biosample database and return Biosample ID + SRA submission: Submit each FASTQ file to SRA database and return an Accession ID</p> <p>We recommend that you use the singularity or docker profile if possible, and only use conda when containers are not an option.</p>"},{"location":"user-guide/wastewater_guide/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Review Nextflow Getting Started if you have never used Nextflow before</li> <li>Install Nextflow</li> <li>Clone the TOSTADAS GitHub repository: <code>git clone https://github.com/CDCgov/tostadas.git</code></li> <li>Register for an NCBI Center Account</li> <li>Create an NCBI Bioproject. Link to the NWSS umbrella Bioproject (PRJNA747181).</li> </ul>"},{"location":"user-guide/wastewater_guide/#2-fill-out-metadata-for-all-samples","title":"2. Fill out Metadata for all samples","text":"<p>Download the Excel template for wastewater metadata and fill out following the examples in the sheet. Rename your file.</p>"},{"location":"user-guide/wastewater_guide/#3-fill-out-the-submission-config-file","title":"3. Fill out the submission config file","text":"<p>Add your center information to this configuration file. Make sure for Biosample package you enter <code>SARS-CoV-2.wwsurv.1.0</code>. Rename the file as needed, but make sure you keep it in the conf/ directory.</p>"},{"location":"user-guide/wastewater_guide/#4-test-your-set-up-with-the-test-profile","title":"4. Test your set up with the test profile","text":"<p>Run the following command to test your setup <code>nextflow run main.nf -profile nwss,test,[docker,singularity,conda]</code></p>"},{"location":"user-guide/wastewater_guide/#5-run-a-test-with-real-data","title":"5. Run a test with real data","text":"<p>Add a few of your actual samples to the Excel metadata sheet and submit these to the test server. NCBI provides a test server to validate the sftp connection before submitting to production. </p> <p><code>nextflow run main.nf -profile nwss,&lt;docker|singularity|conda&gt; --meta_path &lt;path/to/metadata_file.xlsx&gt; --submission_config &lt;path/to/submission_config.yaml&gt; --outdir &lt;path/to/outdir&gt; --dry_run false</code></p>"},{"location":"user-guide/wastewater_guide/#6-submit-small-sample-to-production-server","title":"6. Submit small sample to production server","text":"<p><code>nextflow run main.nf -profile nwss,&lt;docker|singularity|conda&gt; --meta_path &lt;path/to/metadata_file.xlsx&gt; --submission_config &lt;path/to/submission_config.yaml&gt; --outdir &lt;path/to/outdir&gt; --prod_submission true --dry_run false</code></p>"},{"location":"user-guide/wastewater_guide/#7-submit-all-samples-to-production-server","title":"7. Submit all samples to production server","text":"<p>Update your metadata path to point to all of your samples for submissions <code>nextflow run main.nf -profile nwss,&lt;docker|singularity|conda&gt; --meta_path &lt;path/to/metadata_file.xlsx&gt; --submission_config &lt;path/to/submission_config.yaml&gt; --outdir &lt;path/to/outdir&gt; --prod_submission true --dry_run false</code></p>"},{"location":"user-guide/wastewater_guide/#8-troubleshooting","title":"8. Troubleshooting","text":"<p>View the docs</p>"}]}